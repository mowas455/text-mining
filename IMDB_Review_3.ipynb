{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "IMDB Review-3.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mowas455/text-mining/blob/master/IMDB_Review_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import modules"
      ],
      "metadata": {
        "id": "vkG2nDWLRtLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Modules for data manipulation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Modules for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "\n",
        "# Tools for preprocessing input data\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Tools for creating ngrams and vectorizing input data\n",
        "from gensim.models import Word2Vec, Phrases\n",
        "\n",
        "# Tools for building a model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Dropout, Bidirectional\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Tools for assessing the quality of model prediction\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "import os\n",
        "for file in os.listdir(\"../input\"):\n",
        "    print(file)"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2022-03-10T14:06:47.544361Z",
          "iopub.execute_input": "2022-03-10T14:06:47.544763Z",
          "iopub.status.idle": "2022-03-10T14:06:51.296800Z",
          "shell.execute_reply.started": "2022-03-10T14:06:47.544704Z",
          "shell.execute_reply": "2022-03-10T14:06:51.295715Z"
        },
        "trusted": true,
        "id": "37XaG6EyRtLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set some matplotlib configs for visualization"
      ],
      "metadata": {
        "id": "gDjZpTr-RtLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SMALL_SIZE = 12\n",
        "MEDIUM_SIZE = 14\n",
        "BIG_SIZE = 16\n",
        "LARGE_SIZE = 20\n",
        "\n",
        "params = {\n",
        "    'figure.figsize': (16, 8),\n",
        "    'font.size': SMALL_SIZE,\n",
        "    'xtick.labelsize': MEDIUM_SIZE,\n",
        "    'ytick.labelsize': MEDIUM_SIZE,\n",
        "    'legend.fontsize': BIG_SIZE,\n",
        "    'figure.titlesize': LARGE_SIZE,\n",
        "    'axes.titlesize': MEDIUM_SIZE,\n",
        "    'axes.labelsize': BIG_SIZE\n",
        "}\n",
        "plt.rcParams.update(params)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-10T14:06:51.301972Z",
          "iopub.execute_input": "2022-03-10T14:06:51.304839Z",
          "iopub.status.idle": "2022-03-10T14:06:51.316726Z",
          "shell.execute_reply.started": "2022-03-10T14:06:51.304718Z",
          "shell.execute_reply": "2022-03-10T14:06:51.315462Z"
        },
        "trusted": true,
        "id": "ItePoNwbRtLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the dataset\n",
        "df_reviews = pd.read_json('../input/imdb-spoiler-dataset/IMDB_reviews.json', lines=True)\n",
        "df_details = pd.read_json('../input/imdb-spoiler-dataset/IMDB_movie_details.json',lines =True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-10T14:06:51.322257Z",
          "iopub.execute_input": "2022-03-10T14:06:51.322578Z",
          "iopub.status.idle": "2022-03-10T14:07:13.401318Z",
          "shell.execute_reply.started": "2022-03-10T14:06:51.322526Z",
          "shell.execute_reply": "2022-03-10T14:07:13.400308Z"
        },
        "trusted": true,
        "id": "cDa6HSmYRtLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import data\n",
        "Importing the existing datasets and also importing the IMDB dataset from another source. It helps us to increase maximal accuracy of our model from ~87% to 90+%."
      ],
      "metadata": {
        "id": "TQyP2A-QRtLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_part_data(df, num_reviews):\n",
        "    \n",
        "    num_pos_reviews = df[df[\"is_spoiler\"]== True].shape[0]  # Number of spoilers in the dataset\n",
        "    num_neg_reviews = df[df[\"is_spoiler\"]== False].shape[0] # Number of Non-spoilers in the dataset\n",
        "    \n",
        "    fraction_pos = num_reviews/num_pos_reviews  # fraction of spoiler reviews to be returned\n",
        "    fraction_neg = num_reviews/num_neg_reviews  # fraction of non-spoiler reviews to be returned \n",
        "\n",
        "    df_pos = df[df['is_spoiler'] == True].sample(frac = fraction_pos, random_state = 2)\n",
        "    df_neg = df[df['is_spoiler'] == False].sample(frac = fraction_neg, random_state = 2)\n",
        "\n",
        "    df_re = pd.concat([df_pos, df_neg])  # join the True and False dataset\n",
        "    df_re = df_re.reset_index(drop=True)  # mix the index values\n",
        "    df_re.loc[(df_re['is_spoiler'] == True) ,'is_spoiler'] =1\n",
        "    df_re.loc[(df_re['is_spoiler'] == False) ,'is_spoiler'] =0\n",
        "    return df_re[[\"movie_id\",\"review_text\",\"review_summary\",\"is_spoiler\"]]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-10T14:07:47.644715Z",
          "iopub.execute_input": "2022-03-10T14:07:47.645170Z",
          "iopub.status.idle": "2022-03-10T14:07:47.661820Z",
          "shell.execute_reply.started": "2022-03-10T14:07:47.645079Z",
          "shell.execute_reply": "2022-03-10T14:07:47.660646Z"
        },
        "trusted": true,
        "id": "U4waTEHfRtLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = get_part_data(df_reviews, 10000)\n",
        "all_reviews = np.array(data.review_text)\n",
        "print('Total number of reviews:', len(all_reviews))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-10T14:07:49.240972Z",
          "iopub.execute_input": "2022-03-10T14:07:49.241343Z",
          "iopub.status.idle": "2022-03-10T14:07:49.634608Z",
          "shell.execute_reply.started": "2022-03-10T14:07:49.241270Z",
          "shell.execute_reply": "2022-03-10T14:07:49.632196Z"
        },
        "trusted": true,
        "id": "uvdnvR8PRtLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_data = pd.concat((train_data, additional_data[additional_data.sentiment != -1]),\n",
        "#                        axis=0, ignore_index=True)\n",
        "# train_data.info()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-10T14:08:41.856247Z",
          "iopub.execute_input": "2022-03-10T14:08:41.856801Z",
          "iopub.status.idle": "2022-03-10T14:08:41.861959Z",
          "shell.execute_reply.started": "2022-03-10T14:08:41.856723Z",
          "shell.execute_reply": "2022-03-10T14:08:41.860883Z"
        },
        "trusted": true,
        "id": "3zLKUuemRtLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check class balance"
      ],
      "metadata": {
        "id": "HOM4FoVPRtLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.hist(train_data[train_data.sentiment == 1].sentiment,\n",
        "#          bins=2, color='green', label='Positive')\n",
        "# plt.hist(train_data[train_data.sentiment == 0].sentiment,\n",
        "#          bins=2, color='blue', label='Negative')\n",
        "# plt.title('Classes distribution in the train data', fontsize=LARGE_SIZE)\n",
        "# plt.xticks([])\n",
        "# plt.xlim(-0.5, 2)\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-10T14:08:42.580548Z",
          "iopub.execute_input": "2022-03-10T14:08:42.580914Z",
          "iopub.status.idle": "2022-03-10T14:08:42.588772Z",
          "shell.execute_reply.started": "2022-03-10T14:08:42.580862Z",
          "shell.execute_reply": "2022-03-10T14:08:42.587693Z"
        },
        "trusted": true,
        "id": "yroONsNaRtLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_review(raw_review: str) -> str:\n",
        "    # 1. Remove HTML\n",
        "    review_text = BeautifulSoup(raw_review, \"lxml\").get_text()\n",
        "    # 2. Remove non-letters\n",
        "    letters_only = REPLACE_WITH_SPACE.sub(\" \", review_text)\n",
        "    # 3. Convert to lower case\n",
        "    lowercase_letters = letters_only.lower()\n",
        "    return lowercase_letters\n",
        "\n",
        "\n",
        "def lemmatize(tokens: list) -> list:\n",
        "    # 1. Lemmatize\n",
        "    tokens = list(map(lemmatizer.lemmatize, tokens))\n",
        "    lemmatized_tokens = list(map(lambda x: lemmatizer.lemmatize(x, \"v\"), tokens))\n",
        "    # 2. Remove stop words\n",
        "    meaningful_words = list(filter(lambda x: not x in stop_words, lemmatized_tokens))\n",
        "    return meaningful_words\n",
        "\n",
        "\n",
        "def preprocess(review: str, total: int, show_progress: bool = True) -> list:\n",
        "    if show_progress:\n",
        "        global counter\n",
        "        counter += 1\n",
        "        print('Processing... %6i/%6i'% (counter, total), end='\\r')\n",
        "    # 1. Clean text\n",
        "    review = clean_review(review)\n",
        "    # 2. Split into individual words\n",
        "    tokens = word_tokenize(review)\n",
        "    # 3. Lemmatize\n",
        "    lemmas = lemmatize(tokens)\n",
        "    # 4. Join the words back into one string separated by space,\n",
        "    # and return the result.\n",
        "    return lemmas"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-10T14:08:43.512023Z",
          "iopub.execute_input": "2022-03-10T14:08:43.512374Z",
          "iopub.status.idle": "2022-03-10T14:08:43.521427Z",
          "shell.execute_reply.started": "2022-03-10T14:08:43.512313Z",
          "shell.execute_reply": "2022-03-10T14:08:43.520267Z"
        },
        "trusted": true,
        "id": "vYLcYRdYRtLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counter = 0\n",
        "REPLACE_WITH_SPACE = re.compile(r'[^A-Za-z\\s]')\n",
        "stop_words = set(stopwords.words(\"english\")) \n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-10T14:08:48.780935Z",
          "iopub.execute_input": "2022-03-10T14:08:48.781349Z",
          "iopub.status.idle": "2022-03-10T14:08:48.794920Z",
          "shell.execute_reply.started": "2022-03-10T14:08:48.781267Z",
          "shell.execute_reply": "2022-03-10T14:08:48.793538Z"
        },
        "trusted": true,
        "id": "nVKNgpSFRtLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_reviews = np.array(list(map(lambda x: preprocess(x, len(all_reviews)), all_reviews)))\n",
        "counter = 0"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-10T14:08:49.762205Z",
          "iopub.execute_input": "2022-03-10T14:08:49.762534Z",
          "iopub.status.idle": "2022-03-10T14:11:13.472751Z",
          "shell.execute_reply.started": "2022-03-10T14:08:49.762466Z",
          "shell.execute_reply": "2022-03-10T14:11:13.471669Z"
        },
        "trusted": true,
        "id": "Vq4kVwHSRtLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = data\n",
        "X_train_data = all_reviews\n",
        "Y_train_data = train_data.is_spoiler.values"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-10T14:11:56.291259Z",
          "iopub.execute_input": "2022-03-10T14:11:56.291652Z",
          "iopub.status.idle": "2022-03-10T14:11:56.297683Z",
          "shell.execute_reply.started": "2022-03-10T14:11:56.291591Z",
          "shell.execute_reply": "2022-03-10T14:11:56.296595Z"
        },
        "trusted": true,
        "id": "d3nslXUZRtLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_data['review_lenght'] = np.array(list(map(len, X_train_data)))\n",
        "# median = train_data['review_lenght'].median()\n",
        "# mean = train_data['review_lenght'].mean()\n",
        "# mode = train_data['review_lenght'].mode()[0]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-10T14:01:09.361749Z",
          "iopub.status.idle": "2022-03-10T14:01:09.362454Z"
        },
        "trusted": true,
        "id": "OL0_o32dRtLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fig, ax = plt.subplots()\n",
        "# sb.distplot(train_data['review_lenght'], bins=train_data['review_lenght'].max(),\n",
        "#             hist_kws={\"alpha\": 0.9, \"color\": \"blue\"}, ax=ax,\n",
        "#             kde_kws={\"color\": \"black\", 'linewidth': 3})\n",
        "# ax.set_xlim(left=0, right=np.percentile(train_data['review_lenght'], 95))\n",
        "# ax.set_xlabel('Words in review')\n",
        "# ymax = 0.014\n",
        "# plt.ylim(0, ymax)\n",
        "# ax.plot([mode, mode], [0, ymax], '--', label=f'mode = {mode:.2f}', linewidth=4)\n",
        "# ax.plot([mean, mean], [0, ymax], '--', label=f'mean = {mean:.2f}', linewidth=4)\n",
        "# ax.plot([median, median], [0, ymax], '--',\n",
        "#         label=f'median = {median:.2f}', linewidth=4)\n",
        "# ax.set_title('Words per review distribution', fontsize=20)\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-10T14:01:09.363273Z",
          "iopub.status.idle": "2022-03-10T14:01:09.363948Z"
        },
        "trusted": true,
        "id": "SFOnLQstRtLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "bigrams = Phrases(sentences=all_reviews)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-10T14:12:00.398623Z",
          "iopub.execute_input": "2022-03-10T14:12:00.398995Z",
          "iopub.status.idle": "2022-03-10T14:12:09.164643Z",
          "shell.execute_reply.started": "2022-03-10T14:12:00.398915Z",
          "shell.execute_reply": "2022-03-10T14:12:09.163592Z"
        },
        "trusted": true,
        "id": "K6814LDKRtLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "trigrams = Phrases(sentences=bigrams[all_reviews])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-10T14:12:09.167210Z",
          "iopub.execute_input": "2022-03-10T14:12:09.167948Z",
          "iopub.status.idle": "2022-03-10T14:12:37.480980Z",
          "shell.execute_reply.started": "2022-03-10T14:12:09.167887Z",
          "shell.execute_reply": "2022-03-10T14:12:37.479662Z"
        },
        "trusted": true,
        "id": "QRV0emCJRtLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can use gensim's phrases to find bigrams or trigrams"
      ],
      "metadata": {
        "id": "v_aI4pU5RtLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(bigrams['space station near the solar system'.split()])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-10T14:12:37.482455Z",
          "iopub.execute_input": "2022-03-10T14:12:37.483170Z",
          "iopub.status.idle": "2022-03-10T14:12:37.490065Z",
          "shell.execute_reply.started": "2022-03-10T14:12:37.483023Z",
          "shell.execute_reply": "2022-03-10T14:12:37.489021Z"
        },
        "trusted": true,
        "id": "9yy0poI0RtLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "embedding_vector_size = 256\n",
        "trigrams_model = Word2Vec(\n",
        "    sentences = trigrams[bigrams[all_reviews]],\n",
        "    size = embedding_vector_size,\n",
        "    min_count=3, window=5, workers=4)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-10T14:12:37.492234Z",
          "iopub.execute_input": "2022-03-10T14:12:37.493131Z",
          "iopub.status.idle": "2022-03-10T14:17:12.865427Z",
          "shell.execute_reply.started": "2022-03-10T14:12:37.493041Z",
          "shell.execute_reply": "2022-03-10T14:17:12.864201Z"
        },
        "trusted": true,
        "id": "Gc3LYDjpRtLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vocabulary size:\", len(trigrams_model.wv.vocab))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-10T14:17:12.867147Z",
          "iopub.execute_input": "2022-03-10T14:17:12.867478Z",
          "iopub.status.idle": "2022-03-10T14:17:12.876071Z",
          "shell.execute_reply.started": "2022-03-10T14:17:12.867403Z",
          "shell.execute_reply": "2022-03-10T14:17:12.874703Z"
        },
        "trusted": true,
        "id": "aRAZf6bhRtLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now we can use gensim's word2vec model to build a word embedding. Also we can use the word2vec model to define most similar words, calculate diffence between the words, etc."
      ],
      "metadata": {
        "id": "x7CT6B1uRtLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trigrams_model.wv.most_similar('galaxy')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-10T14:17:12.878155Z",
          "iopub.execute_input": "2022-03-10T14:17:12.878815Z",
          "iopub.status.idle": "2022-03-10T14:17:12.985694Z",
          "shell.execute_reply.started": "2022-03-10T14:17:12.878751Z",
          "shell.execute_reply": "2022-03-10T14:17:12.984590Z"
        },
        "trusted": true,
        "id": "EGkDSY0kRtLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trigrams_model.wv.doesnt_match(['galaxy', 'starship', 'planet', 'dog'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-10T14:17:12.987508Z",
          "iopub.execute_input": "2022-03-10T14:17:12.988178Z",
          "iopub.status.idle": "2022-03-10T14:17:12.997002Z",
          "shell.execute_reply.started": "2022-03-10T14:17:12.988116Z",
          "shell.execute_reply": "2022-03-10T14:17:12.995532Z"
        },
        "trusted": true,
        "id": "1VXLjUtHRtLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "def vectorize_data(data, vocab: dict) -> list:\n",
        "    print('Vectorize sentences...', end='\\r')\n",
        "    keys = list(vocab.keys())\n",
        "    filter_unknown = lambda word: vocab.get(word, None) is not None\n",
        "    encode = lambda review: list(map(keys.index, filter(filter_unknown, review)))\n",
        "    vectorized = list(map(encode, data))\n",
        "    print('Vectorize sentences... (done)')\n",
        "    return vectorized\n",
        "\n",
        "print('Convert sentences to sentences with ngrams...', end='\\r')\n",
        "X_data = trigrams[bigrams[X_train_data]]\n",
        "print('Convert sentences to sentences with ngrams... (done)')\n",
        "input_length = 150\n",
        "X_pad = pad_sequences(\n",
        "    sequences=vectorize_data(X_data, vocab=trigrams_model.wv.vocab),\n",
        "    maxlen=input_length,\n",
        "    padding='post')\n",
        "print('Transform sentences to sequences... (done)')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-10T14:17:12.999418Z",
          "iopub.execute_input": "2022-03-10T14:17:13.000356Z",
          "iopub.status.idle": "2022-03-10T14:21:11.807889Z",
          "shell.execute_reply.started": "2022-03-10T14:17:13.000283Z",
          "shell.execute_reply": "2022-03-10T14:21:11.806663Z"
        },
        "trusted": true,
        "id": "yjZE39BORtLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_pad,\n",
        "    Y_train_data,\n",
        "    test_size=0.05,\n",
        "    shuffle=True,\n",
        "    random_state=42)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-10T14:21:11.809293Z",
          "iopub.execute_input": "2022-03-10T14:21:11.809656Z",
          "iopub.status.idle": "2022-03-10T14:21:11.825658Z",
          "shell.execute_reply.started": "2022-03-10T14:21:11.809591Z",
          "shell.execute_reply": "2022-03-10T14:21:11.824561Z"
        },
        "trusted": true,
        "id": "goSuz_wKRtLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(embedding_matrix: np.ndarray, input_length: int):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(\n",
        "        input_dim = embedding_matrix.shape[0],\n",
        "        output_dim = embedding_matrix.shape[1], \n",
        "        input_length = input_length,\n",
        "        weights = [embedding_matrix],\n",
        "        trainable=False))\n",
        "    model.add(Bidirectional(LSTM(128, recurrent_dropout=0.1)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Dense(64))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "model = build_model(\n",
        "    embedding_matrix=trigrams_model.wv.vectors,\n",
        "    input_length=input_length)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-10T14:23:22.241462Z",
          "iopub.execute_input": "2022-03-10T14:23:22.241995Z",
          "iopub.status.idle": "2022-03-10T14:23:25.299618Z",
          "shell.execute_reply.started": "2022-03-10T14:23:22.241905Z",
          "shell.execute_reply": "2022-03-10T14:23:25.298773Z"
        },
        "trusted": true,
        "id": "vFR0qBuIRtLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(\n",
        "    x=X_train,\n",
        "    y=y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    batch_size=100,\n",
        "    epochs=2)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-10T14:24:40.728865Z",
          "iopub.execute_input": "2022-03-10T14:24:40.729206Z",
          "iopub.status.idle": "2022-03-10T14:28:25.441418Z",
          "shell.execute_reply.started": "2022-03-10T14:24:40.729149Z",
          "shell.execute_reply": "2022-03-10T14:28:25.440327Z"
        },
        "trusted": true,
        "id": "pc-kb256RtLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(y_true, y_pred, ax, class_names, vmax=None,\n",
        "                          normed=True, title='Confusion matrix'):\n",
        "    matrix = confusion_matrix(y_true,y_pred)\n",
        "    if normed:\n",
        "        matrix = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]\n",
        "    sb.heatmap(matrix, vmax=vmax, annot=True, square=True, ax=ax,\n",
        "               cmap=plt.cm.Blues_r, cbar=False, linecolor='black',\n",
        "               linewidths=1, xticklabels=class_names)\n",
        "    ax.set_title(title, y=1.20, fontsize=16)\n",
        "    #ax.set_ylabel('True labels', fontsize=12)\n",
        "    ax.set_xlabel('Predicted labels', y=1.10, fontsize=12)\n",
        "    ax.set_yticklabels(class_names, rotation=0)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-10T14:29:03.825419Z",
          "iopub.execute_input": "2022-03-10T14:29:03.825815Z",
          "iopub.status.idle": "2022-03-10T14:29:03.833843Z",
          "shell.execute_reply.started": "2022-03-10T14:29:03.825754Z",
          "shell.execute_reply": "2022-03-10T14:29:03.832436Z"
        },
        "trusted": true,
        "id": "uElmgpAFRtLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "y_train_pred = model.predict_classes(X_train)\n",
        "y_test_pred = model.predict_classes(X_test)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-10T14:29:12.831281Z",
          "iopub.execute_input": "2022-03-10T14:29:12.831744Z",
          "iopub.status.idle": "2022-03-10T14:31:30.380197Z",
          "shell.execute_reply.started": "2022-03-10T14:29:12.831671Z",
          "shell.execute_reply": "2022-03-10T14:31:30.379026Z"
        },
        "trusted": true,
        "id": "u6CQn-d9RtLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (axis1, axis2) = plt.subplots(nrows=1, ncols=2)\n",
        "plot_confusion_matrix(y_train, y_train_pred, ax=axis1,\n",
        "                      title='Confusion matrix (train data)',\n",
        "                      class_names=['Positive', 'Negative'])\n",
        "plot_confusion_matrix(y_test, y_test_pred, ax=axis2,\n",
        "                      title='Confusion matrix (test data)',\n",
        "                      class_names=['Positive', 'Negative'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-10T14:31:30.384056Z",
          "iopub.execute_input": "2022-03-10T14:31:30.384327Z",
          "iopub.status.idle": "2022-03-10T14:31:31.334278Z",
          "shell.execute_reply.started": "2022-03-10T14:31:30.384267Z",
          "shell.execute_reply": "2022-03-10T14:31:31.333282Z"
        },
        "trusted": true,
        "id": "kCniRrn7RtLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (axis1, axis2) = plt.subplots(nrows=1, ncols=2, figsize=(16,6))\n",
        "\n",
        "# summarize history for accuracy\n",
        "axis1.plot(history.history['acc'], label='Train', linewidth=3)\n",
        "axis1.plot(history.history['val_acc'], label='Validation', linewidth=3)\n",
        "axis1.set_title('Model accuracy', fontsize=16)\n",
        "axis1.set_ylabel('accuracy')\n",
        "axis1.set_xlabel('epoch')\n",
        "axis1.legend(loc='upper left')\n",
        "\n",
        "# summarize history for loss\n",
        "axis2.plot(history.history['loss'], label='Train', linewidth=3)\n",
        "axis2.plot(history.history['val_loss'], label='Validation', linewidth=3)\n",
        "axis2.set_title('Model loss', fontsize=16)\n",
        "axis2.set_ylabel('loss')\n",
        "axis2.set_xlabel('epoch')\n",
        "axis2.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-10T14:34:03.207013Z",
          "iopub.execute_input": "2022-03-10T14:34:03.207709Z",
          "iopub.status.idle": "2022-03-10T14:34:03.830497Z",
          "shell.execute_reply.started": "2022-03-10T14:34:03.207368Z",
          "shell.execute_reply": "2022-03-10T14:34:03.829489Z"
        },
        "trusted": true,
        "id": "GpRfwJeYRtLN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}